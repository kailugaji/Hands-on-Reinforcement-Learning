#!/usr/bin/env python3
# -*- coding=utf-8 -*-
# Policy Gradientsâ€”â€”REINFORCE
# The CartPole example
# https://www.cnblogs.com/kailugaji/
import gym
import ptan
import numpy as np
from tensorboardX import SummaryWriter

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

GAMMA = 0.99 # æŠ˜æ‰£ç‡
LEARNING_RATE = 0.01 # å­¦ä¹ ç‡
EPISODES_TO_TRAIN = 4 # how many complete episodes we will use for training

# æ„å»ºç½‘ç»œ
class PGN(nn.Module):
    def __init__(self, input_size, n_actions):
        # input_sizeï¼šè¾“å…¥çŠ¶æ€ç»´åº¦ï¼Œhidden_sizeï¼šéšå±‚ç»´åº¦=128ï¼Œn_actionsï¼šè¾“å‡ºåŠ¨ä½œç»´åº¦
        super(PGN, self).__init__()

        self.net = nn.Sequential(
            nn.Linear(input_size, 128), # å…¨è¿æ¥å±‚ï¼Œéšå±‚é¢„è®¾ä¸º128ç»´åº¦
            nn.ReLU(),
            nn.Linear(128, n_actions) # å…¨è¿æ¥å±‚
        )

    def forward(self, x):
        return self.net(x) # æœªç”¨softmax

# Calculate the reward from the end of the local reward list.
# Indeed, the last step of the episode will have a total reward equal to its local reward.
# The step before the last will have the total reward of r(ğ‘¡âˆ’1) + gamma * r(ğ‘¡)
# sum(gamma^t * reward)
def calc_qvals(rewards):
    res = []
    sum_r = 0.0
    for r in reversed(rewards): # reversed: è¿”å›åå‘çš„è¿­ä»£å™¨å¯¹è±¡
        sum_r *= GAMMA # gamma * r(ğ‘¡), r(t): the total reward for the previous steps
        sum_r += r # r(ğ‘¡âˆ’1) + gamma * r(ğ‘¡), r(t-1): the local reward
        res.append(sum_r) # the discounted total reward
    return list(reversed(res)) # a list of rewards for the whole episode


if __name__ == "__main__":
    env = gym.make("CartPole-v0") # åˆ›å»ºæ¸¸æˆç¯å¢ƒ
    writer = SummaryWriter(comment="-cartpole-reinforce")

    net = PGN(env.observation_space.shape[0], env.action_space.n) # 4(çŠ¶æ€)->128->2(åŠ¨ä½œ)
    # print(net)

    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor, apply_softmax=True)
    # PolicyAgentï¼šè¿ç»­
    # make a decision about actions for every observation (ä¾æ¦‚ç‡)
    # apply_softmax=Trueï¼šç½‘ç»œè¾“å‡ºå…ˆç»è¿‡softmaxè½¬åŒ–æˆæ¦‚ç‡ï¼Œå†ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­è¿›è¡ŒéšæœºæŠ½æ ·
    # float32_preprocessorï¼šreturns the observation as float64 instead of the float32 required by PyTorch
    exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)
    # è¿”å›è¿è¡Œè®°å½•ä»¥ç”¨äºè®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºæ ¼å¼ä¸ºï¼š(state, action, reward, last_state)
    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE) # Adamä¼˜åŒ–

    total_rewards = [] # the total rewards for the episodes
    step_idx = 0 # è¿­ä»£æ¬¡æ•°/è½®æ•°
    done_episodes = 0 # å±€æ•°ï¼Œå‡ å±€æ¸¸æˆ

    batch_episodes = 0
    batch_states, batch_actions, batch_qvals = [], [], []
    cur_rewards = [] # local rewards for the episode being currently played

    for step_idx, exp in enumerate(exp_source): # (state, action, reward, last_state)
        batch_states.append(exp.state) # çŠ¶æ€
        batch_actions.append(int(exp.action)) # åŠ¨ä½œ
        cur_rewards.append(exp.reward) # å³æ—¶å¥–åŠ±

        if exp.last_state is None: # ä¸€å±€æ¸¸æˆç»“æŸ
            batch_qvals.extend(calc_qvals(cur_rewards)) # the discounted total rewards
            cur_rewards.clear()
            batch_episodes += 1

        # handle new rewards
        new_rewards = exp_source.pop_total_rewards() # è¿”å›ä¸€å±€æ¸¸æˆè¿‡åçš„total_rewords
        if new_rewards:
            done_episodes += 1 # æ¸¸æˆå±€æ•°(å›åˆæ•°)
            reward = new_rewards[0]
            total_rewards.append(reward) # the total rewards for the episodes
            mean_rewards = float(np.mean(total_rewards[-100:])) # å¹³å‡å¥–åŠ±
            print("ç¬¬%dæ¬¡: ç¬¬%då±€æ¸¸æˆç»“æŸ, å¥–åŠ±ä¸º%6.2f, å¹³å‡å¥–åŠ±ä¸º%6.2f" % (step_idx, done_episodes, reward, mean_rewards))
            writer.add_scalar("reward", reward, step_idx)
            writer.add_scalar("reward_100", mean_rewards, step_idx)
            writer.add_scalar("episodes", done_episodes, step_idx)
            if mean_rewards > 50: # æœ€å¤§æœŸæœ›å¥–åŠ±é˜ˆå€¼ï¼Œåªæœ‰å½“mean_rewards > 50æ—¶æ‰ç»“æŸæ¸¸æˆ
                print("ç»è¿‡%dè½®å®Œæˆ%då±€æ¸¸æˆ!" % (step_idx, done_episodes))
                break

        if batch_episodes < EPISODES_TO_TRAIN: # how many complete episodes we will use for training
            continue

        # train
        optimizer.zero_grad()
        states_v = torch.FloatTensor(batch_states)
        batch_actions_t = torch.LongTensor(batch_actions)
        batch_qvals_v = torch.FloatTensor(batch_qvals) # the discounted batch total rewards

        logits_v = net(states_v) # è¾“å…¥çŠ¶æ€ï¼Œè¾“å‡ºQ(s, a)å€¼
        log_prob_v = F.log_softmax(logits_v, dim=1) # è¾“å‡ºåŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡åˆ†å¸ƒ log Ï€(s, a)
        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]
        # max sum(gamma^t * reward) * log Ï€(s, a)
        loss_v = -log_prob_actions_v.mean() # min

        loss_v.backward()
        optimizer.step()

        batch_episodes = 0
        batch_states.clear()
        batch_actions.clear()
        batch_qvals.clear()

    writer.close()
